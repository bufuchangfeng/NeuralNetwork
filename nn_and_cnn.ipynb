{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "from dataset import load_mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.cuda.Device(0).use()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_label, test_label = load_mnist()\n",
    "\n",
    "train_data = train_data.reshape(-1, 1, 28, 28)\n",
    "test_data = test_data.reshape(-1, 1, 28, 28)\n",
    "\n",
    "train_data, valid_data, train_label, valid_label = train_test_split(train_data, train_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(valid_data[6].reshape(28, 28))\n",
    "plt.show()\n",
    "valid_label[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_one_hot(y, n_class):\n",
    "    one_hot = cp.zeros((y.shape[0], n_class))\n",
    "    for i in range(len(y)):\n",
    "        one_hot[i][int(y[i])] = 1\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.params = {}\n",
    "        self.params['W'] = cp.random.randn(input_dim, output_dim) / cp.sqrt(input_dim)\n",
    "        self.params['b'] = cp.random.randn(output_dim)\n",
    "        \n",
    "        self.grads = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.x = x\n",
    "        \n",
    "        out = cp.dot(x, self.params['W']) + self.params['b']    \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \n",
    "        self.grads['W'] = cp.dot(self.x.T, dout)\n",
    "        self.grads['b'] = cp.sum(dout, axis=0)\n",
    "        \n",
    "        return cp.dot(dout, self.params['W'].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.params = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        \n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithCrossEntropyLoss:\n",
    "    def __init__(self):\n",
    "        self.params = None\n",
    "    \n",
    "    def forward(self, out, y):\n",
    "        '''\n",
    "            out: output of last fully connected layer\n",
    "            y: true label\n",
    "        '''\n",
    "        \n",
    "        batch_size = out.shape[0]\n",
    "        \n",
    "        max_val = cp.max(out, axis=1).reshape(-1, 1)\n",
    "        exp_out = cp.exp(out - max_val)\n",
    "        sum_exp_out = cp.sum(exp_out, axis=1).reshape(-1, 1)\n",
    "        out = exp_out / sum_exp_out\n",
    "        \n",
    "        self.out = out\n",
    "        self.y = y\n",
    "        \n",
    "        log_out = cp.log(out + 1e-7)\n",
    "    \n",
    "        loss = cp.sum(-log_out * y)\n",
    "            \n",
    "        return loss / batch_size, out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return (self.out - self.y) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        '''\n",
    "            kernel size is a tuple: (height, width)\n",
    "            \n",
    "            out_channels: it is equal to number of filters\n",
    "            \n",
    "            in_channels: number of matrix in each filter\n",
    "            \n",
    "            padding: is omitted\n",
    "        '''\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        \n",
    "        '''\n",
    "            padding it is omitted\n",
    "        '''\n",
    "        self.padding = 0\n",
    "        \n",
    "        self.params = {}\n",
    "        \n",
    "        self.params['W'] = cp.random.randn(out_channels, in_channels, kernel_size[0], kernel_size[1])\n",
    "        \n",
    "        self.params['b'] = cp.random.randn(out_channels)\n",
    "        \n",
    "        self.grads = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            x: (batch_size, in_channels, height, width)\n",
    "        '''\n",
    "        \n",
    "        self.x_shape = x.shape\n",
    "        \n",
    "        self.batch_size, in_channels, in_h, in_w = x.shape\n",
    "        \n",
    "        out_h = 1 + int((in_h + 2*self.padding - self.kernel_size[0]) / self.stride)\n",
    "        out_w = 1 + int((in_w + 2*self.padding - self.kernel_size[1]) / self.stride)\n",
    "        \n",
    "        self.out_h, self.out_w = out_h, out_w\n",
    "        self.in_h, self.in_w = in_h, in_w\n",
    "        \n",
    "        im2col_x = []\n",
    "        \n",
    "        for b in range(self.batch_size):\n",
    "            for i in range(out_h):\n",
    "                for j in range(out_w):\n",
    "                    h_start = i * self.stride\n",
    "                    h_end = h_start + self.kernel_size[0]\n",
    "                    w_start = j * self.stride\n",
    "                    w_end = w_start + self.kernel_size[1]\n",
    "\n",
    "                    current_region = x[b, :, h_start:h_end, w_start:w_end]\n",
    "\n",
    "                    current_region = current_region.reshape(1, -1)\n",
    "\n",
    "                    im2col_x.append(current_region)\n",
    "        \n",
    "        im2col_x = cp.array(im2col_x)\n",
    "\n",
    "        im2col_x = im2col_x.squeeze(1)\n",
    "\n",
    "        \n",
    "        self.im2col_x = im2col_x\n",
    "        self.W = self.params['W'].reshape(self.out_channels, -1).T\n",
    "        \n",
    "        out = cp.dot(im2col_x, self.W)\n",
    "        \n",
    "        out = out + self.params['b']\n",
    "\n",
    "        # this line\n",
    "        self.out_shape = out.shape\n",
    "        \n",
    "        _out = []\n",
    "        for b in range(self.batch_size):\n",
    "            \n",
    "            row_start = out_h * out_w * b\n",
    "            row_end = out_h * out_w * (b + 1)\n",
    "            \n",
    "            _out_b = []\n",
    "            \n",
    "            for oc in range(self.out_channels):\n",
    "                region = out[row_start:row_end, oc].reshape(out_h, out_w)\n",
    "                \n",
    "                _out_b.append(region)\n",
    "            _out.append(_out_b)\n",
    "        \n",
    "        _out = cp.array(_out)\n",
    "        \n",
    "        return _out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        \n",
    "        self.grads['b'] = cp.sum(cp.sum(dout, axis=(-1, -2)), axis=0).reshape(self.out_channels)\n",
    "\n",
    "        '''\n",
    "            shape of grads: (batch_size, out_h, out_w, out_channels)\n",
    "        '''\n",
    "        grads = dout.transpose(0, 2, 3, 1).reshape(self.out_shape)\n",
    "        \n",
    "        self.grads['W'] = cp.dot(self.im2col_x.T, grads)\n",
    "        \n",
    "        self.grads['W'] = self.grads['W'].T.reshape(self.params['W'].shape)\n",
    "        \n",
    "        self.dout = cp.dot(grads, self.W.T)\n",
    "        \n",
    "        self.grads_x = cp.zeros(self.x_shape)\n",
    "        \n",
    "        self.dout = self.dout.reshape(self.dout.shape[0], self.in_channels, int(self.dout.shape[1] / self.in_channels))\n",
    "        \n",
    "        self.dout = self.dout.reshape(self.batch_size, self.out_h, self.out_w, \n",
    "                                      self.in_channels, self.kernel_size[0], self.kernel_size[1])\n",
    "\n",
    "        for h in range(self.out_h):\n",
    "            h_start = h * self.stride\n",
    "            h_end = h_start + self.kernel_size[0]\n",
    "            for w in range(self.out_w):\n",
    "                w_start = w * self.stride\n",
    "                w_end = w_start + self.kernel_size[1]\n",
    "                \n",
    "                self.grads_x[:, :, h_start:h_end, w_start:w_end] += self.dout[:, h, w]\n",
    "                \n",
    "        return self.grads_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ -0.72043991,  -3.31373979,  -8.42731572],\n",
       "         [  7.27480558,  15.57900214,   1.85605171],\n",
       "         [ -1.10265072,  -8.67050987,   1.37808621]],\n",
       "\n",
       "        [[  0.20853502,   2.3436287 ,   2.90548026],\n",
       "         [ -6.93055036,   0.28773803,   1.46510664],\n",
       "         [-12.06518582,   3.32938683,   1.71117612]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Conv2d(2, 2, (2, 2), 1)\n",
    "\n",
    "x = cp.random.randn(1, 2, 3, 3)\n",
    "\n",
    "out = c.forward(x)\n",
    "\n",
    "c.backward(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2d:\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        \n",
    "        '''\n",
    "            kernel size is a tuple: (height, width)\n",
    "        '''\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.params = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size, in_channels, in_h, in_w = x.shape\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        out_h = int(1 + (in_h - self.kernel_size[0]) / self.stride)\n",
    "        out_w = int(1 + (in_w - self.kernel_size[1]) / self.stride)\n",
    "        \n",
    "        self.out_h, self.out_w = out_h, out_w\n",
    "        \n",
    "        im2col_x = []\n",
    "        \n",
    "        for i in range(in_channels):\n",
    "            for j in range(out_h):\n",
    "                for k in range(out_w):\n",
    "                    h_start = j * self.stride\n",
    "                    h_end = h_start + self.kernel_size[0]\n",
    "                    w_start = k * self.stride\n",
    "                    w_end = w_start + self.kernel_size[1]\n",
    "                    \n",
    "                    current_region = x[:, i, h_start:h_end, w_start:w_end]\n",
    "                    \n",
    "                    current_region = current_region.reshape(batch_size, -1)\n",
    "                    \n",
    "                    im2col_x.append(current_region)\n",
    "                    \n",
    "        im2col_x = cp.array(im2col_x)\n",
    "        \n",
    "        im2col_x = im2col_x.transpose(1, 0, 2)\n",
    "\n",
    "        self.x_shape = x.shape\n",
    "        self.im2col_x_shape = im2col_x.shape\n",
    "        \n",
    "        out = cp.max(im2col_x, axis=2)\n",
    "        \n",
    "        self.arg_max = cp.argmax(im2col_x, axis=2)\n",
    "        \n",
    "        out = out.reshape(batch_size, in_channels, out_h, out_w)\n",
    "\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "    \n",
    "        self.dout = cp.zeros(self.x_shape)\n",
    "            \n",
    "        dout = dout.reshape(self.batch_size, -1)\n",
    "            \n",
    "        for b in range(self.batch_size):\n",
    "                \n",
    "            index = 0\n",
    "                \n",
    "            for i in range(self.in_channels):\n",
    "                for j in range(self.out_h):\n",
    "                    for k in range(self.out_w):\n",
    "                        h_start = j * self.stride                \n",
    "                        w_start = k * self.stride\n",
    "\n",
    "                        p = h_start + int(self.arg_max[b][index] /  self.kernel_size[0])\n",
    "                        q = w_start + self.arg_max[b][index] % self.kernel_size[1]\n",
    "\n",
    "                        self.dout[b, i, p, q] += dout[b, index]\n",
    "\n",
    "                        index += 1\n",
    "            \n",
    "        return self.dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.        , 0.60906548, 0.        ],\n",
       "         [1.80856837, 0.        , 0.        ],\n",
       "         [0.        , 0.        , 1.37122347]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 1.34386209, 0.        ],\n",
       "         [0.        , 1.60921718, 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.        , 0.        , 0.        ],\n",
       "         [1.6215562 , 0.        , 2.52840667],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [2.00241194, 0.        , 4.66574918],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.        , 0.        , 0.        ],\n",
       "         [0.        , 2.28393851, 0.        ],\n",
       "         [0.89565669, 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 2.71360916, 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 2.18691032, 0.        ]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = MaxPool2d((2, 2), 1)\n",
    "\n",
    "x = cp.random.randn(3, 2, 3, 3)\n",
    "\n",
    "out = m.forward(x)\n",
    "\n",
    "m.backward(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.layers.append(Linear(784, 400))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Linear(400, 200))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Linear(200, 100))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Linear(100, 10))\n",
    "        self.layers.append(SoftmaxWithCrossEntropyLoss())\n",
    "\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        y = label_to_one_hot(y, 10)\n",
    "        \n",
    "        x = x.reshape(batch_size, 784)\n",
    "        \n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return self.layers[-1].forward(x, y)\n",
    "    \n",
    "    \n",
    "    def backward(self):\n",
    "        dout = 1\n",
    "        for layer in self.layers[::-1]:\n",
    "            dout = layer.backward(dout) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.layers.append(Conv2d(in_channels=1, out_channels=3, kernel_size=(3, 3), stride=2))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(MaxPool2d(kernel_size=(2, 2), stride=1))\n",
    "        self.layers.append(Conv2d(in_channels=3, out_channels=6, kernel_size=(3, 3), stride=1))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(MaxPool2d(kernel_size=(2, 2), stride=1))\n",
    "        self.layers.append(Linear(input_dim=486, output_dim=10))\n",
    "        self.layers.append(SoftmaxWithCrossEntropyLoss())\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        y = label_to_one_hot(y, 10)\n",
    "        \n",
    "        split_index = 6\n",
    "        \n",
    "        for layer in self.layers[:split_index]:\n",
    "            x = layer.forward(x)\n",
    "        x = x.reshape(batch_size, -1)\n",
    "        \n",
    "        x = self.layers[-2].forward(x)\n",
    "        \n",
    "        return self.layers[-1].forward(x, y)\n",
    "        \n",
    "        \n",
    "    def backward(self):\n",
    "        dout = 1\n",
    "        for layer in self.layers[::-1]:\n",
    "            dout = layer.backward(dout)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp(object):\n",
    "    def __init__(self, learning_rate, layers):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.layers = layers\n",
    "\n",
    "        self.s = [None for l in layers]\n",
    "        \n",
    "        self.beta = 0.9\n",
    "        \n",
    "    def step(self):\n",
    "        \n",
    "          for i in range(len(self.layers)):\n",
    "                \n",
    "                if self.layers[i].params is not None:\n",
    "                    \n",
    "                    layer = self.layers[i]\n",
    "                    \n",
    "                    if self.s[i] is None:\n",
    "                        self.s[i] = {}\n",
    "                        for key in layer.params.keys():\n",
    "                            self.s[i][key] = cp.zeros(layer.params[key].shape)\n",
    "\n",
    "                    for key in layer.params.keys():\n",
    "                        self.s[i][key] = self.beta * self.s[i][key] + (1 - self.beta) * layer.grads[key] * layer.grads[key]\n",
    "                        layer.params[key] -= self.learning_rate * layer.grads[key] / cp.sqrt(self.s[i][key] + 1e-10) \n",
    "\n",
    "class Adam(object):\n",
    "    def __init__(self, learning_rate, layers):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "        self.v = [None for l in layers]\n",
    "        self.s = [None for l in layers]\n",
    "        \n",
    "        self.beta_1 = 0.9\n",
    "        self.beta_2 = 0.999\n",
    "        \n",
    "        self.t = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \n",
    "        for i in range(len(self.layers)):\n",
    "            \n",
    "            if self.layers[i].params is not None:\n",
    "            \n",
    "                layer = self.layers[i]\n",
    "\n",
    "                if self.v[i] is None:\n",
    "                    self.v[i] = {}\n",
    "                    for key in layer.params.keys():\n",
    "                        self.v[i][key] = cp.zeros(layer.params[key].shape)\n",
    "\n",
    "                if self.s[i] is None:\n",
    "                    self.s[i] = {}\n",
    "                    for key in layer.params.keys():\n",
    "                        self.s[i][key] = cp.zeros(layer.params[key].shape)\n",
    "\n",
    "                for key in layer.params.keys():\n",
    "\n",
    "                    self.v[i][key] = self.beta_1 * self.v[i][key] + (1 - self.beta_1) * layer.grads[key]\n",
    "                    self.s[i][key] = self.beta_2 * self.s[i][key] + (1 - self.beta_2) * cp.power(layer.grads[key], 2)\n",
    "\n",
    "                    self.v[i][key] = self.v[i][key] / (1 - self.beta_1**(self.t + 1))\n",
    "                    self.s[i][key] = self.s[i][key] / (1 - self.beta_2**(self.t + 1))\n",
    "\n",
    "                    layer.params[key] -= self.learning_rate * self.v[i][key] / (cp.sqrt(self.s[i][key]) + 1e-10)\n",
    "\n",
    "        self.t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_valid(model, epoch, train_data, train_label, valid_data, valid_label, optimizer):\n",
    "    \n",
    "    batch_train_count = int(train_data.shape[0] / batch_size)\n",
    "    batch_train_inputs, batch_train_targets = [], []\n",
    "    for i in range(batch_train_count):\n",
    "        batch_train_inputs.append(train_data[i*batch_size : (i+1)*batch_size])\n",
    "        batch_train_targets.append(train_label[i*batch_size : (i+1)*batch_size])\n",
    "    \n",
    "    batch_valid_count = int(valid_data.shape[0] / batch_size)\n",
    "    batch_valid_inputs, batch_valid_targets = [], []\n",
    "    for i in range(batch_valid_count):\n",
    "        batch_valid_inputs.append(valid_data[i*batch_size : (i+1)*batch_size])\n",
    "        batch_valid_targets.append(valid_label[i*batch_size : (i+1)*batch_size])\n",
    "    \n",
    "    for j in range(batch_train_count):\n",
    "        \n",
    "        inputs = cp.array(batch_train_inputs[j])\n",
    "        targets = cp.array(batch_train_targets[j])\n",
    "        \n",
    "        loss, out = model.forward(inputs, targets)\n",
    "        \n",
    "        if (j + 1) % 25 == 0:\n",
    "            print('loss', loss)\n",
    "        \n",
    "        model.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    correct = 0\n",
    "    for j in range(batch_valid_count):\n",
    "        inputs = cp.array(batch_valid_inputs[j])\n",
    "        targets = cp.array(batch_valid_targets[j])\n",
    "\n",
    "        _, out = model.forward(inputs, targets)\n",
    "\n",
    "        correct += (cp.argmax(out, axis=1) == targets).sum()\n",
    "        \n",
    "        acc = correct / len(valid_data)\n",
    "        \n",
    "    print('epoch: {} valid acc: {}'.format(epoch + 1, acc))\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    batch_test_count = int(test_data.shape[0] / batch_size)\n",
    "    batch_test_inputs, batch_test_targets = [], []\n",
    "    for i in range(batch_test_count):\n",
    "        batch_test_inputs.append(test_data[i*batch_size : (i+1)*batch_size])\n",
    "        batch_test_targets.append(test_label[i*batch_size : (i+1)*batch_size])\n",
    "    correct = 0\n",
    "    for j in range(batch_test_count):\n",
    "        inputs = cp.array(batch_test_inputs[j])\n",
    "        targets = cp.array(batch_test_targets[j])\n",
    "\n",
    "        _, out = model.forward(inputs, targets)\n",
    "\n",
    "        correct += (cp.argmax(out, axis=1) == targets).sum()\n",
    "    print('test acc: {}'.format(correct / 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-3\n",
    "epoch = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN()\n",
    "cnn_optimizer = RMSProp(learning_rate, cnn_model.layers)\n",
    "\n",
    "cnn_acc_list = []\n",
    "\n",
    "'''\n",
    "    由于时间原因，只训练了一个epoch\n",
    "'''\n",
    "for i in range(epoch):\n",
    "    acc = train_and_valid(cnn_model, i, train_data, train_label, valid_data, valid_label, cnn_optimizer)\n",
    "    cnn_acc_list.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.9448\n"
     ]
    }
   ],
   "source": [
    "test(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.811551803309372\n",
      "loss 0.47298521362296664\n",
      "loss 0.23469297725147373\n",
      "loss 0.26928682456370545\n",
      "loss 0.2193222229429806\n",
      "loss 0.159529857548044\n",
      "loss 0.26681572080213517\n",
      "loss 0.15616693582541108\n",
      "loss 0.30508920770849446\n",
      "loss 0.2767181315734044\n",
      "loss 0.19801903743077479\n",
      "loss 0.12251084510977904\n",
      "loss 0.22525864737592838\n",
      "loss 0.09472960457967798\n",
      "loss 0.27505422692232384\n",
      "epoch: 1 valid acc: 0.9529166666666666\n",
      "loss 0.222002967038582\n",
      "loss 0.05690330916686939\n",
      "loss 0.04984532913235174\n",
      "loss 0.05978008656769522\n",
      "loss 0.15501844696745945\n",
      "loss 0.17578960719352185\n",
      "loss 0.10417826684692996\n",
      "loss 0.10157015032319519\n",
      "loss 0.06065950324382946\n",
      "loss 0.1952618100370916\n",
      "loss 0.14779019355600795\n",
      "loss 0.11862714789152566\n",
      "loss 0.15336094636453992\n",
      "loss 0.07357588816430942\n",
      "loss 0.11981603163700416\n",
      "epoch: 2 valid acc: 0.9640833333333333\n",
      "loss 0.14466414876788686\n",
      "loss 0.061849777701592674\n",
      "loss 0.06905535662771954\n",
      "loss 0.03715736835608548\n",
      "loss 0.13710014973607343\n",
      "loss 0.15395131197331463\n",
      "loss 0.04545247659081991\n",
      "loss 0.08029590284576037\n",
      "loss 0.052441534535137155\n",
      "loss 0.13394968302077326\n",
      "loss 0.144275406724554\n",
      "loss 0.15127944618025674\n",
      "loss 0.0911217524787316\n",
      "loss 0.04208301660817876\n",
      "loss 0.09744735347919585\n",
      "epoch: 3 valid acc: 0.9635833333333333\n",
      "loss 0.07270174785919922\n",
      "loss 0.09627458821583193\n",
      "loss 0.04845800094824544\n",
      "loss 0.1347648800590995\n",
      "loss 0.032960060503347405\n",
      "loss 0.14032261420344072\n",
      "loss 0.04587338025887118\n",
      "loss 0.09633051919292573\n",
      "loss 0.1885385381269724\n",
      "loss 0.11833294700504443\n",
      "loss 0.05546154723190522\n",
      "loss 0.04736442540611775\n",
      "loss 0.11476472309899055\n",
      "loss 0.02840508172341275\n",
      "loss 0.0689291680337798\n",
      "epoch: 4 valid acc: 0.9605\n",
      "loss 0.07175337170799793\n",
      "loss 0.07558744544890383\n",
      "loss 0.015062137263123065\n",
      "loss 0.042841503693297395\n",
      "loss 0.08646795022974482\n",
      "loss 0.07997364619353363\n",
      "loss 0.06331284286191396\n",
      "loss 0.09994054059890074\n",
      "loss 0.0335143500406669\n",
      "loss 0.12784860452887453\n",
      "loss 0.0729813676695139\n",
      "loss 0.07251812257395844\n",
      "loss 0.07047306892295249\n",
      "loss 0.04623693818361255\n",
      "loss 0.15391732756120297\n",
      "epoch: 5 valid acc: 0.963\n",
      "loss 0.02403408130572448\n",
      "loss 0.034695445321539536\n",
      "loss 0.10495095428079163\n",
      "loss 0.005750848794831696\n",
      "loss 0.024059224662748097\n",
      "loss 0.088077718602696\n",
      "loss 0.12197033090366427\n",
      "loss 0.05406439697879592\n",
      "loss 0.004819076978280924\n",
      "loss 0.08330398539789283\n",
      "loss 0.01670943416260152\n",
      "loss 0.009339595818255105\n",
      "loss 0.0883585446378965\n",
      "loss 0.04119808242344004\n",
      "loss 0.04851702247960342\n",
      "epoch: 6 valid acc: 0.9564166666666667\n",
      "loss 0.03890832742447213\n",
      "loss 0.05244891413964797\n",
      "loss 0.09921524608802783\n",
      "loss 0.005072256616041704\n",
      "loss 0.043809770266002836\n",
      "loss 0.074806478875694\n",
      "loss 0.018507437091527364\n",
      "loss 0.10071143337706497\n",
      "loss 0.10043923676965821\n",
      "loss 0.09156217861156996\n",
      "loss 0.1254239174029734\n",
      "loss 0.09491715945861548\n",
      "loss 0.041352250996041444\n",
      "loss 0.013688771273074945\n",
      "loss 0.07541982136120606\n",
      "epoch: 7 valid acc: 0.9615\n",
      "loss 0.04920715396585885\n",
      "loss 0.04443934481818068\n",
      "loss 0.08468996460941139\n",
      "loss 0.0012946792538929574\n",
      "loss 0.0015803828936085482\n",
      "loss 0.056329332607306055\n",
      "loss 0.0025154485377747956\n",
      "loss 0.07456188512649863\n",
      "loss 0.01817200044354527\n",
      "loss 0.14257666668182148\n",
      "loss 0.04898803328834554\n",
      "loss 0.09055944900532537\n",
      "loss 0.0934740058531164\n",
      "loss 0.0025759674345189458\n",
      "loss 0.050271590227556756\n",
      "epoch: 8 valid acc: 0.9618333333333333\n",
      "loss 0.06352063423053304\n",
      "loss 0.003478488221600555\n",
      "loss 0.004994888440402801\n",
      "loss 0.007627227776710074\n",
      "loss 0.0919642933475462\n",
      "loss 0.08171720594330045\n",
      "loss 0.008023135566209283\n",
      "loss 0.04663252550303055\n",
      "loss 0.12832463950131087\n",
      "loss 0.21689397645182112\n",
      "loss 0.08682297700312064\n",
      "loss 0.12991654693185276\n",
      "loss 0.008229200237849272\n",
      "loss 0.10344490258530688\n",
      "loss 0.07423784257207777\n",
      "epoch: 9 valid acc: 0.96025\n",
      "loss 0.0737451469537111\n",
      "loss 0.007779415519372633\n",
      "loss 0.0021648543092367225\n",
      "loss 0.04492206204682919\n",
      "loss 0.016148315024416608\n",
      "loss 0.18285833585509303\n",
      "loss 0.004321311172921253\n",
      "loss 0.06081439155951077\n",
      "loss 0.05355126097520052\n",
      "loss 0.18668784059926552\n",
      "loss 0.0009699870814281864\n",
      "loss 0.02349012162530717\n",
      "loss 0.09549428665615248\n",
      "loss 0.006884255775573759\n",
      "loss 0.07693927713259652\n",
      "epoch: 10 valid acc: 0.96575\n",
      "test acc: 0.9721\n"
     ]
    }
   ],
   "source": [
    "fnn_model = FNN()\n",
    "fnn_optimizer = RMSProp(learning_rate, fnn_model.layers)\n",
    "\n",
    "fnn_acc_list = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    acc = train_and_valid(fnn_model, i, train_data, train_label, valid_data, valid_label, fnn_optimizer)\n",
    "    fnn_acc_list.append(acc)\n",
    "    \n",
    "test(fnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
